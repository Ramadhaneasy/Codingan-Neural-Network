{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input values (2 neurons)\n",
    "input_value = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Output values (2 neurons)\n",
    "output = np.array([[0, 1], [1, 0], [1, 0], [0, 1]])\n",
    "\n",
    "# Bobot dan bias untuk input -> hidden (2 -> 3)\n",
    "weights_input_hidden = np.array([[0.2, -0.4, 0.5],\n",
    "                                  [-0.9, 0.3, -0.2]])\n",
    "bias_hidden = np.array([[-0.3, 0.3, 0.7]])\n",
    "\n",
    "# Bobot dan bias untuk hidden -> output (3 -> 2)\n",
    "weights_hidden_output = np.array([[-0.1, 0.8],\n",
    "                                   [0.2, -0.6],\n",
    "                                   [0.5, 0.3]])\n",
    "bias_output = np.array([[0.4, -0.6]])\n",
    "\n",
    "# Membatasi nilai ke maksimum 0.9\n",
    "weights_input_hidden = np.clip(weights_input_hidden, -0.9, 0.9)\n",
    "bias_hidden = np.clip(bias_hidden, -0.9, 0.9)\n",
    "weights_hidden_output = np.clip(weights_hidden_output, -0.9, 0.9)\n",
    "bias_output = np.clip(bias_output, -0.9, 0.9)\n",
    "\n",
    "# Fungsi aktivasi sigmoid dan turunannya\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for epochs in range(10000):\n",
    "    # Forward propagation\n",
    "    # Input to Hidden\n",
    "    hidden_layer_input = np.dot(input_value, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Hidden to Output\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Error\n",
    "    error = output - predicted_output\n",
    "\n",
    "    # Backpropagation\n",
    "    # Output layer\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    # Hidden layer\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    weights_input_hidden += input_value.T.dot(d_hidden_layer) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Membatasi bobot dan bias ke maksimum 0.9 setelah pembaruan\n",
    "    weights_input_hidden = np.clip(weights_input_hidden, -0.9, 0.9)\n",
    "    bias_hidden = np.clip(bias_hidden, -0.9, 0.9)\n",
    "    weights_hidden_output = np.clip(weights_hidden_output, -0.9, 0.9)\n",
    "    bias_output = np.clip(bias_output, -0.9, 0.9)\n",
    "\n",
    "# Print final weights and biases\n",
    "print(\"Final weights (Input -> Hidden):\", weights_input_hidden)\n",
    "print(\"Final bias (Hidden):\", bias_hidden)\n",
    "print(\"Final weights (Hidden -> Output):\", weights_hidden_output)\n",
    "print(\"Final bias (Output):\", bias_output)\n",
    "\n",
    "# Test prediction\n",
    "test_input = np.array([0, 1])\n",
    "hidden_layer_input = np.dot(test_input, weights_input_hidden) + bias_hidden\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "predicted_output = sigmoid(output_layer_input)\n",
    "print(\"Prediction for input [0, 1]:\", predicted_output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
